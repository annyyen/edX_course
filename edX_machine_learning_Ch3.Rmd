---
title: "edX_machine learning (Ch3)"
author: "Anny"
date: "`r Sys.time()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

[Course](https://courses.edx.org/courses/course-v1:HarvardX+PH125.8x+2T2018/course/)

###3. Linear Regression for Prediction, Smoothing, and Working with Matrices Overview
Learn why linear regression is a useful baseline approach but is often insufficiently flexible for more complex analyses, how to smooth noisy data, and how to use matrices for machine learning.<br>
After completing this section, you will be able to:

+ Use **linear regression for prediction** as a baseline approach.

+ Use **logistic regression** for categorical data.

+ Detect trends in noisy data using **smoothing** (also known as **curve fitting** or **low pass filtering**).

+ Convert predictors to **matrices** and outcomes to **vectors** when all predictors are numeric (or can be converted to numerics in a meaningful way).

+ Perform basic **matrix algebra** calculations.

This section has three parts: **linear regression for prediction**, **smoothing**, and **working with matrices**.

####Linear Regression for Prediction
```{r}
#install.packages("HistData")
library(HistData)
library(dplyr)
galton_heights=GaltonFamilies%>%
  filter(childNum==1 & gender=="male")%>%
  select(father,childHeight)%>%
  rename(son=childHeight)
```

```{r}
library(caret)
y=galton_heights$son
test_index=createDataPartition(y,times = 1,p=0.5,list=F)
train_set=galton_heights%>%slice(-test_index)
test_set=galton_heights%>%slice(test_index)

avg=mean(train_set$son)
avg
mean((avg-test_set$son)^2) #R squared loss
```
conditional expectation is equivalent to the regression line<br>
f(x) = E(Y | X = x) = β<sub>0</sub> + β<sub>1</sub>x <br>

```{r}
fit=lm(son~father,data=train_set)
fit$coef

y_hat=fit$coef[1]+fit$coef[2]*test_set$father
mean((y_hat-test_set$son)^2)

```
####Predict function
```{r}
y_hat=predict(fit,test_set)
mean((y_hat-test_set$son)^2)
# ?predict.lm
# ?predict.glm
```
####`r "\U1F440"`Check
**Q1**<br>
Create a data set using the following code:<br>
```{r}
set.seed(1)
n <- 100
Sigma <- 9*matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2)
dat <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) %>%
	data.frame() %>% setNames(c("x", "y"))
```

We will build 100 linear models using the data above and calculate the mean and standard deviation of the combined models. First, set the seed to 1. Within a ```replicate``` loop, <br>
(1) partition the dataset into test and training sets of equal size using ```dat$y``` to generate your indices, <br>
(2) train a linear model predicting ```y``` from ```x```, <br>
(3) generate predictions on the test set, and <br>
(4) calculate the RMSE of that model. Then, report the mean and standard deviation of the RMSEs from all 100 models.<br>
Mean:
```{r}
#Ans
set.seed(1)
rmse <- replicate(100, { #(4)
	test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE) #(1)
	train_set <- dat %>% slice(-test_index)
	test_set <- dat %>% slice(test_index)
	fit <- lm(y ~ x, data = train_set) #(2)
 	y_hat <- predict(fit, newdata = test_set) #(3)
	sqrt(mean((y_hat-test_set$y)^2))
})
mean(rmse)
```
SD:
```{r}
sd(rmse)
```
**Q2**<br>
Now we will repeat the exercise above but using larger datasets. Write a function that takes a size ```n```, then <br>
(1) builds a dataset using the code provided in Q1 but with ```n``` observations instead of 100 and without the ```set.seed(1)```, <br>
(2) runs the ```replicate``` loop that you wrote to answer Q1, which builds 100 linear models and returns a vector of RMSEs, and <br>
(3) calculates the mean and standard deviation. <br>
Set the seed to 1 and then use ```sapply``` or ```map``` to apply this function to ```n <- c(100, 500, 1000, 5000, 10000)```.<br><br>

Hint: You only need to set the seed once before running your function; do not set a seed within your function. Also be sure to use ```sapply``` or ```map``` as you will get different answers running the simulations individually due to setting the seed.<br>
```{r}
#Ans
set.seed(1)
n <- c(100, 500, 1000, 5000, 10000) #(1)
res <- sapply(n, function(n){ 
	Sigma <- 9*matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2)
	dat <- MASS::mvrnorm(n, c(69, 69), Sigma) %>%
		data.frame() %>% setNames(c("x", "y"))
	rmse <- replicate(100, { #(2)
		test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
		train_set <- dat %>% slice(-test_index)
		test_set <- dat %>% slice(test_index)
		fit <- lm(y ~ x, data = train_set)
		y_hat <- predict(fit, newdata = test_set)
		sqrt(mean((y_hat-test_set$y)^2))
	})
	c(avg = mean(rmse), sd = sd(rmse)) #(3)
})

res
```
<font color= VioletRed>Mean, 100: 2.498</font><br>
<font color= VioletRed>SD, 100: 0.118</font><br>
<font color= VioletRed>Mean, 500: 2.72</font><br>
<font color= VioletRed>SD, 500: 0.08</font><br>
<font color= VioletRed>Mean, 1000: 2.5555</font><br>
<font color= VioletRed>SD, 1000: 0.0456</font><br>
<font color= VioletRed>Mean, 5000: 2.6248</font><br>
<font color= VioletRed>SD, 5000: 0.0231</font><br>
<font color= VioletRed>Mean, 10000: 2.6184</font><br>
<font color= VioletRed>SD, 10000: 0.0169</font><br>

**Q3**<br>
What happens to the RMSE as the size of the dataset becomes larger?<br>
<font color= VioletRed>

+ On average, the RMSE does not change much as ```n``` gets larger, but the variability of the RMSE decreases.</font><br><font color= gray>

+ Because of the law of large numbers the RMSE decreases; more data means more precise estimates.

+ ```n = 10000``` is not sufficiently large. To see a decrease in the RMSE we would need to make it larger.

+ The RMSE is not a random variable.</font>

**Q4**<br>
Now repeat the exercise from Q1, this time making the correlation between ```x``` and ```y``` larger, as in the following code:<br>
```{r}
set.seed(1)
n <- 100
Sigma <- 9*matrix(c(1.0, 0.95, 0.95, 1.0), 2, 2)
dat <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) %>%
	data.frame() %>% setNames(c("x", "y"))
```
Note what happens to RMSE - set the seed to 1 as before.<br>
Mean:
```{r}
rmse <- replicate(100, { 
	test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
	train_set <- dat %>% slice(-test_index)
	test_set <- dat %>% slice(test_index)
	fit <- lm(y ~ x, data = train_set)
 	y_hat <- predict(fit, newdata = test_set) 
	sqrt(mean((y_hat-test_set$y)^2))
})
mean(rmse)
```

SD:
```{r}
sd(rmse)
#correct: 0.0624
```
**Q5**<br>
Which of the following best explains why the RMSE in question 4 is so much lower than the RMSE in question 1?
<font color= gray>

+ It is just luck. If we do it again, it will be larger.<br>

+ The central limit theorem tells us that the RMSE is normal.

<font color= VioletRed>

+ When we increase the correlation between ```x``` and ```y```, ```x``` has more predictive power and thus provides a better estimate of ```y```.</font>

+ These are both examples of regression so the RMSE has to be the same.</font>

**Q6**<br>
Create a data set using the following code.<br>
```{r}
set.seed(1)
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.25, 0.75, 0.25, 1.0), 3, 3)
dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %>%
	data.frame() %>% setNames(c("y", "x_1", "x_2"))
```
Note that ```y``` is correlated with both ```x_1``` and ```x_2``` but the two predictors are independent of each other, as seen by ```cor(dat)```.<br><br>

Set the seed to 1, then use the **caret** package to partition into a test and training set of equal size. Compare the RMSE when using just ```x_1```, just ```x_2``` and both ```x_1``` and ```x_2```. Train a linear model for each.<br><br>

Which of the three models performs the best (has the lowest RMSE)?
<font color= gray>

+ ```x_1```

+ ```x_2```</font><font color= VioletRed>

+ ```x_1``` and ```x_2```</font><br>

```{r}
#Ans
set.seed(1)
test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
train_set <- dat %>% slice(-test_index)
test_set <- dat %>% slice(test_index)

fit <- lm(y ~ x_1, data = train_set)
y_hat <- predict(fit, newdata = test_set)
sqrt(mean((y_hat-test_set$y)^2))

fit <- lm(y ~ x_2, data = train_set)
y_hat <- predict(fit, newdata = test_set)
sqrt(mean((y_hat-test_set$y)^2))

fit <- lm(y ~ x_1 + x_2, data = train_set)
y_hat <- predict(fit, newdata = test_set)
sqrt(mean((y_hat-test_set$y)^2))
```
**Q7**<br>
Report the lowest RMSE of the three models tested in Q6.<br>
<font color= VioletRed>The lowest RMSE is for the model that includes ```x_1``` and ```x_2``` as predictors: 0.3070962.</font><br>
**Q8**<br>
Repeat the exercise from q6 but now create an example in which ```x_1``` and ```x_2``` are highly correlated.
```{r}
set.seed(1)
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.95, 0.75, 0.95, 1.0), 3, 3)
dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %>%
	data.frame() %>% setNames(c("y", "x_1", "x_2"))
```
Set the seed to 1, then use the **caret** package to partition into a test and training set of equal size. Compare the RMSE when using just ```x_1```, just ```x_2```, and both ```x_1``` and ```x_2```.<br>
Compare the results from q6 and q8. What can you conclude?<br>
<font color= gray>

+ Unless we include all predictors we have no predictive power.

+ Adding extra predictors improves RMSE regardless of whether the added predictors are correlated with other predictors or not.

+ Adding extra predictors results in over fitting.<font color = VioletRed>

+ Adding extra predictors can improve RMSE substantially, but not when the added predictors are highly correlated with other predictors.</font></font><br>

**Explanation**
```{r}
set.seed(1)
test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
train_set <- dat %>% slice(-test_index)
test_set <- dat %>% slice(test_index)

fit <- lm(y ~ x_1, data = train_set)
y_hat <- predict(fit, newdata = test_set)
sqrt(mean((y_hat-test_set$y)^2))

fit <- lm(y ~ x_2, data = train_set)
y_hat <- predict(fit, newdata = test_set)
sqrt(mean((y_hat-test_set$y)^2))

fit <- lm(y ~ x_1 + x_2, data = train_set)
y_hat <- predict(fit, newdata = test_set)
sqrt(mean((y_hat-test_set$y)^2))
```
When the predictors are highly correlated with each other, adding addtional predictors does not improve the model substantially, thus RMSE stays roughly the same.<br>

####Regression for a Categorical Outcome
```{r}
library(dslabs)
data("heights")
y=heights$height
set.seed(2)
test_index=createDataPartition(y,times = 1,p=0.5,list=F)
train_set=heights%>%slice(-test_index)
test_set=heights%>%slice(test_index)

train_set%>%
  filter(round(height)==66)%>%
  summarize(mean(sex=="Female"))

lm_fit=mutate(train_set, y=as.numeric(sex=="Female"))%>%
  lm(y~height, data=.)

p_hat=predict(lm_fit,test_set)
y_hat=ifelse(p_hat>0.5,"Female","Male")%>%factor()
confusionMatrix(y_hat, test_set$sex)
```
####Logistic Regression
```{r}
glm_fit=train_set%>%
  mutate(y=as.numeric(sex=="Female"))%>%
  glm(y~height,data=.,family = "binomial")

p_hat_logit=predict(glm_fit,newdata = test_set,type="response")

y_hat_logit=ifelse(p_hat_logit>0.5,"Female","Male")%>%factor

confusionMatrix(y_hat_logit,test_set$sex)
```
####Case Study: 2 or 7
```{r, out.width = "400px", echo=FALSE}
knitr::include_graphics("px1.png")
knitr::include_graphics("px2.png")
```

```{r}
data("mnist_27")
```

```{r, out.width = "400px", echo=FALSE}
knitr::include_graphics("px3.png")
knitr::include_graphics("px4.png")
```

```{r}
fit=glm(y~ x_1 + x_2,data=mnist_27$train, family = "binomial")
p_hat=predict(fit,newdata = mnist_27$test)
y_hat=factor(ifelse(p_hat>0.5,7,2))
confusionMatrix(data=y_hat,reference = mnist_27$test$y)
```
```{r}
mnist_27$true_p %>% ggplot(aes(x_1,x_2,fill=p)) +
  geom_raster()
```
```{r}
mnist_27$true_p %>% ggplot(aes(x_1,x_2,z=p,fill=p)) +
  geom_raster() +
  scale_fill_gradientn(colors=c("#F8766D","white","#00BFC4")) +
  stat_contour(breaks=c(0.5),color="black")
```
```{r, out.width = "400px", echo=FALSE}
knitr::include_graphics("px5.png")
knitr::include_graphics("px6.png")
```

####`r "\U1F440"`Check
**Q1**<br>
Define a dataset using the following code:
```{r}
set.seed(2)
make_data <- function(n = 1000, p = 0.5, 
				mu_0 = 0, mu_1 = 2, 
				sigma_0 = 1,  sigma_1 = 1){

y <- rbinom(n, 1, p)
f_0 <- rnorm(n, mu_0, sigma_0)
f_1 <- rnorm(n, mu_1, sigma_1)
x <- ifelse(y == 1, f_1, f_0)
  
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)

list(train = data.frame(x = x, y = as.factor(y)) %>% slice(-test_index),
	test = data.frame(x = x, y = as.factor(y)) %>% slice(test_index))
}
dat <- make_data()
```
Note that we have defined a variable ```x``` that is predictive of a binary outcome ```y```: <br>
```dat$train %>% ggplot(aes(x, color = y)) + geom_density()```.<br><br>
Set the seed to 1, then use the ```make_data``` function defined above to generate 25 different datasets with ```mu_1 <- seq(0, 3, len=25)```. Perform logistic regression on each of the 25 different datasets (predict 1 if p>0.5) and plot accuracy (```res``` in the figures) vs mu_1 (```delta``` in the figures).”<br><br>
Which is the correct plot?<br>
```{r, out.width = "400px", echo=FALSE}
knitr::include_graphics("q1.png") #correct
knitr::include_graphics("q2.png")
knitr::include_graphics("q3.png")
knitr::include_graphics("q4.png")
```

**Explanation**
```{r}
set.seed(1)
delta <- seq(0, 3, len = 25)
res <- sapply(delta, function(d){
	dat <- make_data(mu_1 = d)
	fit_glm <- dat$train %>% glm(y ~ x, family = "binomial", data = .)
	y_hat_glm <- ifelse(predict(fit_glm, dat$test) > 0.5, 1, 0) %>% factor(levels = c(0, 1))
	mean(y_hat_glm == dat$test$y)
})
qplot(delta, res)
```
<br>

####Introduction to Smoothing
```{r}
data("polls_2008")
qplot(day,margin,data=polls_2008)
```
![](s2.png){width=700px}

####Bin Smoothing and Kernels
![](s3.png){width=700px}

<font color=VioletRed>In the poll example, for each day, we would compute the average for the values within a week of the day that we're considering.<br>
The black points are the points that are used to compute the average at those two points.<br>
The blue line represents the average that was computed.</font><br>
```{r}
span=7
fit=with(polls_2008,
         ksmooth(day,margin,x.points = day,kernel = "box",bandwidth = span))
polls_2008 %>% mutate(smooth=fit$y) %>%
  ggplot(aes(day,margin)) +
  geom_point(size=3, alpha=0.5,color="grey") +
  geom_line(aes(day,margin),color="red")
```
```{r}
span=7
fit=with(polls_2008,
         ksmooth(day,margin,x.points = day,kernel = "box",bandwidth = span))
polls_2008 %>% mutate(smooth=fit$y) %>%
  ggplot(aes(day,margin)) +
  geom_point(size=3, alpha=0.5,color="grey") +
  geom_line(aes(day,smooth),color="red")
```

####Local Weighted Regression (loess)
```{r}
total_days=diff(range(polls_2008$day))
span=21/total_days
fit=loess(margin~day,degree=1,span=span,data=polls_2008)
polls_2008 %>% mutate(smooth=fit$fitted) %>%
  ggplot(aes(day,margin)) +
  geom_point(size=3, alpha=0.5,color="grey") +
  geom_line(aes(day,smooth),color="red")
```
![](l1.png){width=700px}
<br><font color=VioletRed>There are three other differences between loess and the typical bin smoother. <br>
The first is that rather than keeping the bin size the same, loess keeps the number of points used in the local fit the same. This number is controlled via the span argument which expects a proportion.<br>
So for example, if N is a number of data points, and the span is 0.5, then for any given X, loess will use ```0.5 * N closest points``` to X for the fit.<br><br>

Another difference is that when fitting a line locally, loess uses a weighted approach. Basically, instead of least squares, we minimize a weighted version:<br>
![](w.png){width=300px}

However, instead of the Gaussian kernel, loess uses a function called the Tukey tri-weight:<br>
![](ttw.png){width=200px}
<br><br>to define weights:
![](w1.png){width=200px}
<br>The kernel for the tri-weight looks like this.<br><br>
![](kernel.png){width=700px}
<br><br>The third difference is that loess has the option of fitting the local model robustly. An iterative algorithm is implemented in which, after fitting a model in one iteration, outliers are detected and down-weighted for the next iteration.<br>
To use this option, use the argument ```family = "symmetric"```.<br><br>

Taylor's theorem also tells us that if you look at a function close enough, it looks like a parabola and that you don't have to look as close as you do for the linear approximation.<br>
This means we can make our windows even larger and fit parabolas instead of lines, so the local model would look like this:<br></font>
![](m1.png){width=400px}
```{r}
polls_2008 %>% ggplot(aes(day,margin)) +
  geom_point() +
  geom_smooth()
```
```{r}
polls_2008 %>% ggplot(aes(day,margin)) +
  geom_point() +
  geom_smooth(color="red",span=0.15,
              method.args=list(degree=1))
```

####`r "\U1F440"`Check
**Q1**<br>
In the Wrangling course of this series, PH125.6x, we used the following code to obtain mortality counts for Puerto Rico for 2015-2018:<br>
```{r}
library(tidyverse)
library(purrr)
library(pdftools)
library(lubridate)    

fn <- system.file("extdata", "RD-Mortality-Report_2015-18-180531.pdf", package="dslabs")
dat <- map_df(str_split(pdf_text(fn), "\n"), function(s){
	s <- str_trim(s)
	header_index <- str_which(s, "2015")[1]
	tmp <- str_split(s[header_index], "\\s+", simplify = TRUE)
	month <- tmp[1]
	header <- tmp[-1]
	tail_index  <- str_which(s, "Total")
	n <- str_count(s, "\\d+")
	out <- c(1:header_index, which(n==1), which(n>=28), tail_index:length(s))
	s[-out] %>%
		str_remove_all("[^\\d\\s]") %>%
		str_trim() %>%
		str_split_fixed("\\s+", n = 6) %>%
		.[,1:5] %>%
		as_data_frame() %>% 
		setNames(c("day", header)) %>%
		mutate(month = month,
			day = as.numeric(day)) %>%
		gather(year, deaths, -c(day, month)) %>%
		mutate(deaths = as.numeric(deaths))
}) %>%
	mutate(month = recode(month, "JAN" = 1, "FEB" = 2, "MAR" = 3, "APR" = 4, "MAY" = 5, "JUN" = 6, 
                          "JUL" = 7, "AGO" = 8, "SEP" = 9, "OCT" = 10, "NOV" = 11, "DEC" = 12)) %>%
	mutate(date = make_date(year, month, day)) %>%
	filter(date <= "2018-05-01")
```
Use the loess function to obtain a smooth estimate of the expected number of deaths as a function of date. Plot this resulting smooth function. Make the span about two months long.<br><br>

Which of the following plots is correct?<br>
```{r, out.width = "200px", echo=FALSE}
knitr::include_graphics("dd1.png")
knitr::include_graphics("dd2.png")
knitr::include_graphics("dd3.png")
knitr::include_graphics("dd4.png")
```

**Explanation**<br>
```{r}
span <- 60 / as.numeric(diff(range(dat$date)))
fit <- dat %>% mutate(x = as.numeric(date)) %>% loess(deaths ~ x, data = ., span = span, degree = 1)
dat %>% mutate(smooth = predict(fit, as.numeric(date))) %>%
	ggplot() +
	geom_point(aes(date, deaths)) +
	geom_line(aes(date, smooth), lwd = 2, col = 2)
```
<br><font color = VioletRed>The second plot uses a shorter span, the third plot uses the entire timespan, and the fourth plot uses a longer span.</font><br>

**Q2**<br>
Work with the same data as in Q1 to plot smooth estimates against day of the year, all on the same plot, but with different colors for each year.<br><br>
Which code produces the desired plot?<br>
```
dat %>% 
	mutate(smooth = predict(fit), day = yday(date), year = as.character(year(date))) %>%
	ggplot(aes(day, smooth, col = year)) +
	geom_line(lwd = 2)
```
```
dat %>% 
	mutate(smooth = predict(fit, as.numeric(date)), day = mday(date), year = as.character(year(date))) %>%
	ggplot(aes(day, smooth, col = year)) +
	geom_line(lwd = 2)
```
```
dat %>% 
	mutate(smooth = predict(fit, as.numeric(date)), day = yday(date), year = as.character(year(date))) %>%
	ggplot(aes(day, smooth)) +
  	geom_line(lwd = 2)
```
<font color = VioletRed>
```
dat %>% 
	mutate(smooth = predict(fit, as.numeric(date)), day = yday(date), year = as.character(year(date))) %>%
	ggplot(aes(day, smooth, col = year)) +
	geom_line(lwd = 2)
```</font><br>

**Q3**<br>
Suppose we want to predict 2s and 7s in the ```mnist_27``` dataset with just the second covariate. Can we do this? On first inspection it appears the data does not have much predictive power.

In fact, if we fit a regular logistic regression the coefficient for ```x_2``` is not significant!

This can be seen using this code:<br>
```{r}
library(broom)
mnist_27$train %>% glm(y ~ x_2, family = "binomial", data = .) %>% tidy()
```
Plotting a scatterplot here is not useful since y is binary:<br>
```{r}
qplot(x_2, y, data = mnist_27$train)
```
<br>Fit a loess line to the data above and plot the results. What do you observe?<br>
<font color=gray>

+ There is no predictive power and the conditional probability is linear.

+ There is no predictive power and the conditional probability is non-linear.

+ There is predictive power and the conditional probability is linear.</font><font color = VioletRed>

+ There is predictive power and the conditional probability is non-linear.</font><br>

```{r}
#Ans
mnist_27$train %>% 
	mutate(y = ifelse(y=="7", 1, 0)) %>%
	ggplot(aes(x_2, y)) + 
	geom_smooth(method = "loess")
```

####Matrices
```{r}
mnist=read_mnist()
class(mnist$train$images)
x=mnist$train$images[1:1000,]
y=mnist$train$labels[1:1000]
```
5 challenges:<br>
1. study the distribution of the total pixel darkness and how it varies by digits.<br>
2. study the variation of each pixel and remove predictors, columns, associated with pixels that don't change much and thus can't provide much information for classification.<br>
3. zero out low values that are likely smudges. First, we're going to look at the distribution of all pixel values, use this to pick a cutoff to define unwritten space, then make anything below that cutoff a 0.<br>
4. binarize the data. We're going to first look at the distribution of all pixel values, use this to pick a cutoff, and distinguish between writing and no writing. Then convert all entries into either zero or one respectively.<br>
5. scale each of the predictors in each entry to have the same average and standard deviation.<br>

####Matrix Notation
In matrix algebra we have three main types of objects: scalars, vectors, and matrices.<br>
```{r}
length(x[1,])
dim(as.matrix(x))
```
####Converting a Vector to a Matrix
```{r}
my_vector=1:15
mat=matrix(my_vector,5,3)
mat
mat_t=matrix(my_vector,3,5,byrow = T)
mat_t
identical(t(mat),mat_t)

matrix(my_vector,5,5) #the product of columns and rows does not match the length of the vector
grid=matrix(x[3,],28,28)
image(1:28,1:28,grid)
image(1:28,1:28,grid[,28:1])
```

####Row and Column Summaries and Apply
```{r}
sums=rowSums(x)
avg=rowMeans(x)
avgs=apply(x,1,mean)
sds=apply(x,2,sd)
```

####Filtering Columns Based on Summaries
```{r}
library(matrixStats)
sds=colSds(x)

#x[,c(351,352)]
#x[c(2,3),]
new_x=x[,colSds(x)>60]
dim(new_x)
class(x[,1])
dim(x[1,])
class(x[,1,drop=F])
dim(x[ ,1,drop=F])
```
####Indexing with Matrices and Binarizing the Data
```{r}
mat=matrix(1:15,5,3)
mat
as.vector(mat)
qplot(as.vector(x),bins=30,color=I("black"))
new_x=x
new_x[new_x<50]=0
mat=matrix(1:15,5,3)
mat[mat<3]=0
mat
mat=matrix(1:15,5,3)
mat[mat>6 & mat<12]=0
mat

bin_x=x
bin_x[bin_x < 255/2]=0
bin_x[bin_x > 255/2]=1
bin_x=(x>255/2)*1

```
![](m3.png){width=600px}

####Vectorization for Matrices and Matrix Algebra Operations
subtract a vector from a matrix.<br>
![](vm.png){width=400px}
```{r, eval=FALSE}
(x - rowMeans(x)) / rowSds(x)
t(t(x) - colMeans(x))
x_mean_0=sweep(x,2,colMeans(x))
x_standardized=sweep(x_mean_0,2,colSds(x),FUN="/")
t(x) %*% x
crossprod(x)
solve(crossprod(x)) #To compute the inverse of a function
qr(x) #qr decomposition
```

####`r "\U1F440"`Check
**Q1**<br>
Which line of code correctly creates a 100 by 10 matrix of randomly generated normal numbers and assigns it to ```x```?
<font color=gray>

+ ```x <- matrix(rnorm(1000), 100, 100)```

+ <font color=VioletRed>```x <- matrix(rnorm(100*10), 100, 10)```</font>

+ ```x <- matrix(rnorm(100*10), 10, 10)```

+ ```x <- matrix(rnorm(100*10), 10, 100)```</font><br>

**Q2**<br>
Write the line of code that would give you the specified information about the matrix ```x``` that you generated in q1. Do not include any spaces in your line of code.<br>
Dimension of ```x```.
```{r}
x <- matrix(rnorm(100*10), 100, 10)
dim(x)
```
Number of rows of ```x```.
```{r}
nrow(x)
dim(x)[1]
```
Number of columns of ```x```.
```{r}
ncol(x)
dim(x)[2]
```

**Q3**<br>
Which of the following lines of code would add the scalar 1 to row 1, the scalar 2 to row 2, and so on, for the matrix ```x```?<br>

+ <font color=VioletRed>```x <- x + seq(nrow(x))```<font color=gray>

+ ```x <- 1:nrow(x)```

+ ```x <- sweep(x, 2, 1:nrow(x),"+")```</font>

+ ```x <- sweep(x, 1, 1:nrow(x),"+")```</font><br>

**Q4**<br>
Which of the following lines of code would add the scalar 1 to column 1, the scalar 2 to column 2, and so on, for the matrix x?<br>
<font color=gray>

+ ```x <- 1:ncol(x)```

+ ```x <- 1:col(x)```

+ <font color=VioletRed>```x <- x <- sweep(x, 2, 1:ncol(x), FUN = "+")```</font>

+ ```x <- -x```</font>

**Q5**<br>
Which code correctly computes the average of each row of ```x```?<br>
<font color=gray>

+ ```mean(x)```

+ ```rowMedians(x)```

+ ```sapply(x,mean)```

+ ```rowSums(x)```</font>

+ <font color=VioletRed>```rowMeans(x)```</font>

Which code correctly computes the average of each column of ```x```?<br>
<font color=gray>

+ ```mean(x)```

+ ```sapply(x,mean)```

+ <font color=VioletRed>```colMeans(x)```</font>

+ ```colMedians(x)```

+ ```colSums(x)```</font>

**Q6**<br>
For each digit in the mnist training data, compute the proportion of pixels that are in the **grey area**, defined as values between 50 and 205. (To visualize this, you can make a boxplot by digit class.)<br><br>
What proportion of pixels are in the grey area overall, defined as values between 50 and 205?<br>
```{r}
#Ans
#mnist <- read_mnist()
y <- rowMeans(mnist$train$images>50 & mnist$train$images<205)
mean(y)
qplot(as.factor(mnist$train$labels), y, geom = "boxplot")
```

<br><br><br>
